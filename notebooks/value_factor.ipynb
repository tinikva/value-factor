{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e22cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from datetime import datetime, date\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, Any, Optional, List, Literal, Union, Callable\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "# Silence all warnings since I reviewed them\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data processing imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "# Data visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Financial data imports\n",
    "import yfinance as yf\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff7524",
   "metadata": {},
   "source": [
    "### Step 1: Read, Clean, and Process Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database_credentials() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Get database credentials from environment variables.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    Dict[str, str]: Database connection parameters\n",
    "    \"\"\"\n",
    "    # Load environment variables from .env file if it exists\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Get credentials with fallbacks to default values\n",
    "    return {\n",
    "        \"user\": os.environ.get(\"DB_USER\"),\n",
    "        \"password\": os.environ.get(\"DB_PASSWORD\"),\n",
    "        \"host\": os.environ.get(\"DB_HOST\"),\n",
    "        \"database\": os.environ.get(\"DB_NAME\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c75b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection() -> Any:\n",
    "    \"\"\"\n",
    "    Create and return a database connection engine.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    sqlalchemy.engine.Engine: SQLAlchemy database engine\n",
    "    \"\"\"\n",
    "    creds = get_database_credentials()\n",
    "    \n",
    "    # Create connection string\n",
    "    conn_str = (\n",
    "        \"mysql+pymysql://{user}:{password}@{host}/{database}\"\n",
    "    ).format(**creds)\n",
    "    \n",
    "    # Build engine\n",
    "    return create_engine(conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d74503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_universe_data(engine: Any) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and analyze universe data.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    engine: sqlalchemy.engine.Engine\n",
    "        Database connection engine\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame: DataFrame containing universe data\n",
    "    \"\"\"\n",
    "    # Load universe data\n",
    "    universe_df = pd.read_sql(\"SELECT * FROM vf_universe;\", engine)\n",
    "    print(f\"Universe shape: {universe_df.shape}\")\n",
    "    \n",
    "    # Analyze universe data\n",
    "    print(f\"Unique exchanges: {universe_df['exchange'].unique()}\")\n",
    "    print(f\"Unique categories: {universe_df['category'].unique()}\")\n",
    "    print(f\"Unique table codes: {universe_df.groupby(['table_code'])['ticker'].nunique()}\")\n",
    "    print(f\"Unique scalemarketcap: {universe_df['scalemarketcap'].unique()}\")\n",
    "    \n",
    "    return universe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c5716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(engine: Any) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and analyze metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    engine: sqlalchemy.engine.Engine\n",
    "        Database connection engine\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame: DataFrame containing metadata\n",
    "    \"\"\"\n",
    "    # Load metadata\n",
    "    metadata_df = pd.read_sql(\"SELECT * FROM vf_metadata;\", engine)\n",
    "    print(f\"Metadata shape: {metadata_df.shape}\")\n",
    "    \n",
    "    # Analyze metadata\n",
    "    print(f\"Unique table codes: {metadata_df['table_code'].unique()}\")\n",
    "    print(metadata_df[metadata_df['table_code'] == 'SF1'][['title', 'description']].drop_duplicates())\n",
    "    print(metadata_df[metadata_df['table_code'] == 'SEP'][['title', 'description']].drop_duplicates())\n",
    "    print(f\"Unique titles: {metadata_df[metadata_df['table_code'] == 'SEP']['title'].unique()}\")\n",
    "    print(f\"Unique titles: {sorted(metadata_df[metadata_df['table_code'] == 'SF1']['title'].unique())}\")\n",
    "    \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fundamentals(engine: Any) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load financial fundamentals data.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    engine: sqlalchemy.engine.Engine\n",
    "        Database connection engine\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame: DataFrame with fundamental data\n",
    "    \"\"\"\n",
    "    # I am choosing TTM with restatements (MRT) since it smooths out seasonal swings and covers the most recent twelve months, \n",
    "    # giving a more stable and timely view than raw quarterly or annual snapshots. Including restatements ensures I'm using the corrected,\n",
    "    #  most accurate historical figures. Excluding restatements risks relying on numbers later proven wrong. \n",
    "    # Applying an appropriate reporting lag to MRT balances signal freshness with look-ahead bias control.\n",
    "    fundamentals_df = pd.read_sql(\n",
    "        \"SELECT * FROM case_study_2.vf_acct_sf1 WHERE dimension='MRT' AND ticker IS NOT NULL;\", \n",
    "        engine\n",
    "    )\n",
    "    \n",
    "    print(f\"Number of unique tickers: {fundamentals_df['ticker'].nunique()}\")\n",
    "    print(f\"Min and max datekey: {fundamentals_df['datekey'].min()}, {fundamentals_df['datekey'].max()}\")\n",
    "    print(f\"Fundamentals shape: {fundamentals_df.shape}\")\n",
    "    \n",
    "    return fundamentals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7032861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_borrow_fees(engine: Any) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load borrow fees data for relevant tickers.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    engine: sqlalchemy.engine.Engine\n",
    "        Database connection engine\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame: DataFrame with borrow fees\n",
    "    \"\"\"\n",
    "    borrow_fees_df = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT * FROM vf_borrow \n",
    "        WHERE `#SYM` IN (\n",
    "            SELECT DISTINCT ticker \n",
    "            FROM case_study_2.vf_acct_sf1 \n",
    "            WHERE ticker IS NOT NULL\n",
    "        );\n",
    "        \"\"\", \n",
    "        engine\n",
    "    )\n",
    "    \n",
    "    return borrow_fees_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1171b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prices(engine: Any) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load historical price data for relevant tickers.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    engine: sqlalchemy.engine.Engine\n",
    "        Database connection engine\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame: DataFrame with price data\n",
    "    \"\"\"\n",
    "    prices_df = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT DISTINCT ticker, date, close, volume \n",
    "        FROM vf_marketdata_sep \n",
    "        WHERE ticker IN (\n",
    "            SELECT DISTINCT ticker \n",
    "            FROM case_study_2.vf_acct_sf1 \n",
    "            WHERE ticker IS NOT NULL\n",
    "        ) \n",
    "        ORDER BY ticker, date;\n",
    "        \"\"\", \n",
    "        engine\n",
    "    )\n",
    "    \n",
    "    print(f\"Prices shape: {prices_df.shape}\")\n",
    "    \n",
    "    return prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1959f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load all datasets needed for analysis.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    Dict[str, Any]: Dictionary containing all loaded datasets\n",
    "    \"\"\"\n",
    "    # Create database connection\n",
    "    engine = create_connection()\n",
    "    \n",
    "    # Load all datasets\n",
    "    data = {\n",
    "        \"engine\": engine,\n",
    "        \"universe\": load_universe_data(engine),\n",
    "        \"metadata\": load_metadata(engine),\n",
    "        \"fundamentals\": load_fundamentals(engine),\n",
    "        \"borrow_fees\": load_borrow_fees(engine),\n",
    "        \"prices\": load_prices(engine)\n",
    "    }\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d4b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_csv(data: Dict[str, Any], output_dir: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Save loaded datasets to CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    data: Dict[str, Any]\n",
    "        Dictionary containing datasets as returned by load_all_data()\n",
    "    output_dir: Optional[str]\n",
    "        Directory to save CSV files (defaults to current directory)\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    dir_path = output_dir or '.'\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    # Save each dataset to CSV\n",
    "    data[\"universe\"].to_csv(f\"{dir_path}/universe.csv\", index=False)\n",
    "    data[\"metadata\"].to_csv(f\"{dir_path}/metadata.csv\", index=False)\n",
    "    data[\"fundamentals\"].to_csv(f\"{dir_path}/fundamentals.csv\", index=False)\n",
    "    data[\"borrow_fees\"].to_csv(f\"{dir_path}/borrow_fees.csv\", index=False)\n",
    "    data[\"prices\"].to_csv(f\"{dir_path}/prices.csv\", index=False)\n",
    "    \n",
    "    print(f\"All datasets saved to {dir_path}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adv(\n",
    "    prices_df: pd.DataFrame, \n",
    "    window: int = 60, \n",
    "    min_periods: int = 20\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate Average Daily Volume (ADV) for each ticker.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    prices_df: pd.DataFrame\n",
    "        Price data with ticker, date, volume columns\n",
    "    window: int, default=60\n",
    "        Rolling window size in days\n",
    "    min_periods: int, default=20\n",
    "        Minimum number of observations required\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame: DataFrame with ADV_60d column added\n",
    "    \"\"\"\n",
    "    df = prices_df.copy()\n",
    "    \n",
    "    # Sort by ticker and date\n",
    "    df = df.sort_values([\"ticker\", \"date\"])\n",
    "    \n",
    "    # Calculate rolling average volume\n",
    "    df[\"ADV_60d\"] = (\n",
    "        df.groupby(\"ticker\")[\"volume\"]\n",
    "        .transform(lambda x: x.rolling(window=window, min_periods=min_periods).mean())\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758b2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fundamentals(\n",
    "    fundamentals_df: pd.DataFrame,\n",
    "    lag_days: int = 60\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare fundamentals data by adding availability dates.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    fundamentals_df: pd.DataFrame\n",
    "        Fundamentals data\n",
    "    lag_days: int, default=60\n",
    "        Number of days to lag availability from datekey\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame: Prepared fundamentals DataFrame\n",
    "    \"\"\"\n",
    "    df = fundamentals_df.copy()\n",
    "    \n",
    "    # Sort by ticker and datekey\n",
    "    # I use the datekey, since it represents the actual SEC‐filing or observation date\n",
    "    # ensuring I'm only ever using information that was available at rebalance time. \n",
    "    # which avoids look-ahead bias and keeps my factor signals properly synchronized with market prices.\n",
    "    df = df.sort_values(by=['ticker', 'datekey'])\n",
    "    df['available_date'] = pd.to_datetime(df['datekey']) + pd.Timedelta(days=lag_days)\n",
    "    df['available_date'] = pd.to_datetime(df['available_date'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0b9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_borrow_fees(borrow_fees_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fill missing borrow fees data using business day reindexing.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    borrow_fees_df: pd.DataFrame\n",
    "        Borrow fees data with date and #SYM columns\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame: Filled borrow fees DataFrame\n",
    "    \"\"\"\n",
    "    # Ensure datetime\n",
    "    df = borrow_fees_df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values(['#SYM','date']).drop_duplicates(['#SYM','date'], keep='last')\n",
    "\n",
    "    filled = []\n",
    "    for sym, grp in df.groupby('#SYM'):\n",
    "        # Define that ticker's own business‐day range\n",
    "        dr = pd.bdate_range(start=grp['date'].min(), end=grp['date'].max())\n",
    "\n",
    "        # reindex and ffill within that window\n",
    "        tmp = (\n",
    "            grp\n",
    "              .set_index('date')\n",
    "              .reindex(dr)     # only days where this symbol existed\n",
    "              .ffill()         # forward-fill fees & availability\n",
    "        )\n",
    "        tmp['#SYM'] = sym   # put symbol back\n",
    "        filled.append(tmp.reset_index().rename(columns={'index':'date'}))\n",
    "\n",
    "    filled_df = pd.concat(filled, ignore_index=True)\n",
    "    \n",
    "    # Calculate borrow cost\n",
    "    filled_df['borrow_cost'] = filled_df.apply(\n",
    "        lambda row: row['avg_fee_rate'] - max(row['avg_rebate_rate'], 0), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return filled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbdf062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_fundamentals_with_prices(\n",
    "    prices_df: pd.DataFrame, \n",
    "    fundamentals_df: pd.DataFrame,\n",
    "    cols_to_include: Optional[List[str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge fundamentals with prices using as-of merge.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    prices_df: pd.DataFrame\n",
    "        Price data with date column as datetime\n",
    "    fundamentals_df: pd.DataFrame\n",
    "        Fundamentals data with available_date column as datetime\n",
    "    cols_to_include: Optional[List[str]]\n",
    "        List of columns to include from fundamentals\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame: Merged DataFrame\n",
    "    \"\"\"\n",
    "    # Ensure dates are datetime\n",
    "    prices = prices_df.copy()\n",
    "    funds = fundamentals_df.copy()\n",
    "    \n",
    "    prices['date'] = pd.to_datetime(prices['date'])\n",
    "    \n",
    "    # Default columns to include if none specified\n",
    "    if cols_to_include is None:\n",
    "        cols_to_include = ['ticker', 'available_date', 'marketcap', \n",
    "                          'roic', 'roa', 'roe', 'currentratio', 'grossmargin', 'de',\n",
    "                          'pe1', 'pb', 'evebitda', 'divyield', 'fcfps']\n",
    "    else:\n",
    "        # Ensure required columns are included\n",
    "        for col in ['ticker', 'available_date']:\n",
    "            if col not in cols_to_include:\n",
    "                cols_to_include.append(col)\n",
    "    \n",
    "    # Sort for merge_asof\n",
    "    prices = prices.sort_values(['date'])\n",
    "    funds = funds.sort_values(['available_date'])[cols_to_include]\n",
    "\n",
    "    # Merge using merge_asof\n",
    "    merged = pd.merge_asof(\n",
    "        prices,\n",
    "        funds,\n",
    "        left_on='date',\n",
    "        right_on='available_date',\n",
    "        by='ticker',\n",
    "        direction='backward'  # \"last available fundamentals on or before each price date\"\n",
    "    )\n",
    "    \n",
    "    return merged.dropna(subset=['available_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ace634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_forward_returns(\n",
    "    df: pd.DataFrame, \n",
    "    period: int = 20,\n",
    "    price_col: str = 'close'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add forward return calculations to a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame with ticker, date and price data\n",
    "    period: int, default=20\n",
    "        Number of days forward for return calculation\n",
    "    price_col: str, default='close'\n",
    "        Column name for price data\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame: DataFrame with forward returns added\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Ensure sorted\n",
    "    result = result.sort_values(['ticker', 'date'])\n",
    "    \n",
    "    # Calculate 1-month forward return\n",
    "    result['return_1m_forward'] = result.groupby('ticker')[price_col] \\\n",
    "        .transform(lambda x: x.shift(-period) / x - 1)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c41d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filters(\n",
    "    df: pd.DataFrame,\n",
    "    min_market_cap: float = 1e9,\n",
    "    min_adv: float = 2e6\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply universe filters for market cap and liquidity.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame with marketcap and ADV_60d columns\n",
    "    min_market_cap: float, default=1e9\n",
    "        Minimum market cap in dollars\n",
    "    min_adv: float, default=2e6\n",
    "        Minimum average daily volume in dollars\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame: Filtered DataFrame\n",
    "    \"\"\"\n",
    "    filtered = df.copy()\n",
    "    \n",
    "    # Apply filters\n",
    "    # Since I don't have any universe membership data I will define the universe based on market cap and ADV\n",
    "    filtered = filtered[filtered[\"marketcap\"] > min_market_cap]  # min $1B market cap\n",
    "    filtered = filtered[filtered[\"ADV_60d\"] > min_adv]           # min $2M daily volume\n",
    "    \n",
    "    print(f\"After filtering: {filtered.shape[0]} rows ({filtered.shape[0]/df.shape[0]:.1%} of original)\")\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf81d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sector_data(\n",
    "    df: pd.DataFrame,\n",
    "    universe_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add sector information to data using merge_asof.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame with ticker and date columns\n",
    "    universe_df: pd.DataFrame\n",
    "        Universe DataFrame with sector information\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame: DataFrame with sector information added\n",
    "    \"\"\"\n",
    "    # Extract sector information\n",
    "    sectors_df = universe_df[universe_df['table_code'] == 'SEP'][\n",
    "        ['sector', 'ticker', 'lastupdated', 'firstadded']\n",
    "    ].drop_duplicates().copy()\n",
    "    \n",
    "    # Prepare for merge\n",
    "    sectors_df['effective_date'] = pd.to_datetime(sectors_df['lastupdated'])\n",
    "    sectors_df = sectors_df.sort_values(['effective_date'])\n",
    "    \n",
    "    df = df.sort_values(['date'])\n",
    "    \n",
    "    # Ensure date columns are datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # As-of merge to get sector data\n",
    "    result = pd.merge_asof(\n",
    "        df,\n",
    "        sectors_df[['ticker', 'sector', 'effective_date']],\n",
    "        left_on='date',\n",
    "        right_on='effective_date',\n",
    "        by='ticker',\n",
    "        direction='nearest'\n",
    "    )\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    if 'effective_date' in result.columns:\n",
    "        result.drop(columns=['effective_date'], inplace=True)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99869669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(load_from_csv: bool = False, data_dir: str = 'data') -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process all data through the entire pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    load_from_csv: bool, default=False\n",
    "        Whether to load data from CSV files instead of database\n",
    "    data_dir: str, default='data'\n",
    "        Directory containing CSV files if load_from_csv=True\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    Dict[str, pd.DataFrame]: Dictionary of processed datasets\n",
    "    \"\"\"\n",
    "    if load_from_csv:\n",
    "        # Load from CSV files\n",
    "        universe_df = pd.read_csv(f'{data_dir}/universe.csv')\n",
    "        prices_df = pd.read_csv(f'{data_dir}/prices.csv')\n",
    "        fundamentals_df = pd.read_csv(f'{data_dir}/fundamentals.csv')\n",
    "        borrow_fees_df = pd.read_csv(f'{data_dir}/borrow_fees.csv')\n",
    "    else:\n",
    "        # Load from database\n",
    "        data = load_all_data()\n",
    "        universe_df = data['universe']\n",
    "        prices_df = data['prices']\n",
    "        fundamentals_df = data['fundamentals']\n",
    "        borrow_fees_df = data['borrow_fees']\n",
    "        \n",
    "        # Optionally save raw data\n",
    "        save_data_to_csv(data, data_dir)\n",
    "    \n",
    "    # Filter prices to correct universe\n",
    "    prices_df = prices_df[prices_df['ticker'].isin(\n",
    "        universe_df[universe_df['table_code'] == 'SEP']['ticker'].unique()\n",
    "    )]\n",
    "    \n",
    "    # Calculate ADV\n",
    "    prices_df = calculate_adv(prices_df)\n",
    "    \n",
    "    # Data validation\n",
    "    print(\"Checking for duplicates:\")\n",
    "    print(f\"Prices duplicates: {prices_df.shape[0] - prices_df[['ticker', 'date']].drop_duplicates().shape[0]}\")\n",
    "    print(f\"Fundamentals duplicates: {fundamentals_df.shape[0] - fundamentals_df[['ticker', 'datekey']].drop_duplicates().shape[0]}\")\n",
    "    \n",
    "    # Prepare fundamentals\n",
    "    fundamentals_df = prepare_fundamentals(fundamentals_df)\n",
    "    \n",
    "    # Process borrow fees\n",
    "    borrow_fees_filled = fill_borrow_fees(borrow_fees_df)\n",
    "    \n",
    "    # Define columns of interest\n",
    "    quality_cols = ['roic', 'roa', 'roe', 'currentratio', 'grossmargin', 'de']\n",
    "    value_cols = [\"pe1\", \"pb\", 'evebitda', 'divyield', 'fcfps']\n",
    "    cols_to_include = ['ticker', 'available_date', 'marketcap'] + quality_cols + value_cols\n",
    "    \n",
    "    # Merge fundamentals with prices\n",
    "    value_metrics = merge_fundamentals_with_prices(prices_df, fundamentals_df, cols_to_include)\n",
    "    \n",
    "    # Add forward returns\n",
    "    value_metrics = add_forward_returns(value_metrics)\n",
    "    \n",
    "    # Apply filters\n",
    "    filtered_metrics = apply_filters(value_metrics)\n",
    "    \n",
    "    # Add sector information\n",
    "    value_metrics_w_sector = add_sector_data(filtered_metrics, universe_df)\n",
    "    \n",
    "    # Save processed data\n",
    "    value_metrics_w_sector.to_csv(f'{data_dir}/value_metrics_processed.csv', index=False)\n",
    "    borrow_fees_filled.to_csv(f'{data_dir}/borrow_fees_filled.csv', index=False)\n",
    "    \n",
    "    return {\n",
    "        \"universe\": universe_df,\n",
    "        \"prices\": prices_df,\n",
    "        \"fundamentals\": fundamentals_df,\n",
    "        \"borrow_fees\": borrow_fees_df,\n",
    "        \"borrow_fees_filled\": borrow_fees_filled,\n",
    "        \"value_metrics\": value_metrics,\n",
    "        \"value_metrics_filtered\": filtered_metrics,\n",
    "        \"value_metrics_w_sector\": value_metrics_w_sector\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca00c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed_data = process_data()\n",
    "processed_data = process_data(load_from_csv=True, data_dir='data')\n",
    "value_metrics = processed_data[\"value_metrics_filtered\"]\n",
    "enhanced_metrics = processed_data[\"value_metrics_w_sector\"]\n",
    "borrow_fees = processed_data[\"borrow_fees_filled\"]\n",
    "prices = processed_data[\"prices\"]\n",
    "universe = processed_data[\"universe\"]\n",
    "sectors_df = universe[universe['table_code'] == 'SEP'][\n",
    "        ['sector', 'ticker', 'lastupdated', 'firstadded']\n",
    "    ].drop_duplicates().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d1d9a",
   "metadata": {},
   "source": [
    "### Step 2: Construct Traditional Value/Growth Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2148520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_value_growth_factor(\n",
    "    value_metrics: pd.DataFrame,\n",
    "    date: pd.Timestamp,\n",
    "    gross_target: float = 100e6\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Implement traditional Value/Growth factor with a $100MM gross exposure target.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    value_metrics : DataFrame\n",
    "        Must include ['date','ticker','pe1','pb','evebitda','divyield','fcfps'].\n",
    "    date : Timestamp\n",
    "        Rebalance date.\n",
    "    gross_target : float\n",
    "        Total gross exposure (|long| + |short|) in dollars.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    factor_df : DataFrame\n",
    "        Columns: ['ticker','value_score','position','weight','dollar_position']\n",
    "    \"\"\"\n",
    "    current_data = value_metrics[value_metrics['date'] == date].copy()\n",
    "\n",
    "    # Winsorize and fill missing\n",
    "    for col in [\"pe1\",\"pb\",\"evebitda\",\"divyield\",\"fcfps\"]:\n",
    "        lo, hi = current_data[col].quantile([0.05,0.95])\n",
    "        current_data[col] = current_data[col].clip(lo, hi)\n",
    "        med = current_data[col].median()\n",
    "        current_data[col] = current_data[col].fillna(med)\n",
    "\n",
    "    # Invert ratios where higher = more value\n",
    "    current_data['pe_ratio_inv']     = 1 / current_data['pe1']\n",
    "    current_data['pb_ratio_inv']     = 1 / current_data['pb']\n",
    "    current_data['evebitda_inv']     = 1 / current_data['evebitda']\n",
    "\n",
    "    # Define value columns and drop any remaining NaNs\n",
    "    value_cols = ['pe_ratio_inv','pb_ratio_inv','divyield','evebitda_inv','fcfps']\n",
    "    current_data.dropna(subset=value_cols, inplace=True)\n",
    "\n",
    "    # Compute cross‐sectional ranks\n",
    "    for col in value_cols:\n",
    "        current_data[f\"{col}_rank\"] = current_data[col].rank(ascending=False, pct=True)\n",
    "\n",
    "    # Composite score = average rank\n",
    "    rank_cols = [f\"{col}_rank\" for col in value_cols]\n",
    "    current_data['value_score'] = current_data[rank_cols].mean(axis=1)\n",
    "\n",
    "    # Long top quintile, short bottom quintile\n",
    "    current_data['bin'] = pd.qcut(current_data['value_score'], 5, labels=False)\n",
    "    current_data['position'] = 0\n",
    "    current_data.loc[current_data['bin'] == 4, 'position'] =  1\n",
    "    current_data.loc[current_data['bin'] == 0, 'position'] = -1\n",
    "\n",
    "    # Equal‐weight each side (sums to +1 / -1)\n",
    "    n_long  = (current_data['position'] ==  1).sum()\n",
    "    n_short = (current_data['position'] == -1).sum()\n",
    "\n",
    "    current_data['weight'] = 0.0\n",
    "    if n_long:\n",
    "        current_data.loc[current_data['position'] ==  1, 'weight'] =  1.0 / n_long\n",
    "    if n_short:\n",
    "        current_data.loc[current_data['position'] == -1, 'weight'] = -1.0 / n_short\n",
    "\n",
    "    # Scale to gross_target\n",
    "    gross_current = current_data['weight'].abs().sum()    # should be ~2.0\n",
    "    scale = gross_target / gross_current                 # e.g. 100e6 / 2 = 50e6\n",
    "\n",
    "    current_data['dollar_position'] = current_data['weight'] * scale\n",
    "    # Optionally redefine weight as fraction of gross\n",
    "    current_data['weight'] = current_data['dollar_position'] / gross_target\n",
    "\n",
    "    return current_data[['ticker','value_score','position','weight','dollar_position']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876d25e",
   "metadata": {},
   "source": [
    "### Step 3: Construct Enhanced Value/Growth Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a9892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the report for more details on the drawbacks of the traditional approach that the enhanced approach aims to address\n",
    "def beta_neutralize(pos_df: pd.DataFrame,\n",
    "                    beta_df: pd.DataFrame,\n",
    "                    date: pd.Timestamp,\n",
    "                    gross_target: float = 100e6\n",
    "                   ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adjusts portfolio positions to achieve beta neutrality while maintaining target gross exposure.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pos_df : pd.DataFrame\n",
    "        Portfolio positions with at least columns ['ticker', 'dollar_position']\n",
    "    beta_df : pd.DataFrame\n",
    "        DataFrame containing beta values with at least columns ['date', 'ticker', 'beta']\n",
    "    date : pd.Timestamp\n",
    "        The specific date for which to apply beta neutralization\n",
    "    gross_target : float, default=100e6\n",
    "        Target gross exposure in dollars (e.g., 100 million)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The adjusted portfolio with the same columns as the input plus a new 'weight' column,\n",
    "        where sum(dollar_position * beta) ≈ 0 and sum(abs(dollar_position)) ≈ gross_target\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The adjustment uses the formula:\n",
    "        adjusted_position_i = position_i + λ * beta_i\n",
    "    where λ = -Σ(position_i * beta_i) / Σ(beta_i²)\n",
    "    \n",
    "    Missing beta values are filled with 1.0 (market beta).\n",
    "    \"\"\"\n",
    "    # Merge in each ticker’s beta for this date\n",
    "    betas = beta_df.loc[beta_df['date']==date.strftime('%Y-%m-%d'), ['ticker','beta']]\n",
    "    pos = pos_df.merge(betas, on='ticker', how='left').fillna({'beta': 1.0})\n",
    "\n",
    "    # Compute the adjustment factor λ\n",
    "    #    We want Σ (pos_i + λ * β_i) * β_i = 0\n",
    "    #    => Σ pos_i*β_i + λ Σ β_i^2 = 0  ⇒  λ = – Σ pos_i*β_i / Σ β_i^2\n",
    "    num   = (pos['dollar_position'] * pos['beta']).sum()\n",
    "    denom = (pos['beta'] ** 2).sum()\n",
    "    lam   = - num / denom\n",
    "\n",
    "    # Shift each dollar_position by λ * beta_i\n",
    "    pos['dollar_position'] += lam * pos['beta']\n",
    "\n",
    "    # Rescale back to exact gross_target\n",
    "    #    (gross = Σ |dollar_position|)\n",
    "    gross = pos['dollar_position'].abs().sum()\n",
    "    pos['weight'] = pos['dollar_position'] / gross_target\n",
    "    pos['dollar_position'] *= (gross_target / gross)\n",
    "\n",
    "    # Return with original columns plus the new weight\n",
    "    return pos[pos_df.columns.drop('weight').tolist() + ['weight']]\n",
    "\n",
    "def enhanced_value_growth_factor(\n",
    "    enhanced_metrics: pd.DataFrame,\n",
    "    borrow_costs_df: pd.DataFrame,\n",
    "    beta_df: pd.DataFrame,\n",
    "    date: pd.Timestamp,\n",
    "    lookback_months: int = 12,\n",
    "    max_positions: int = 400,\n",
    "    sector_neutrality: bool = True,\n",
    "    gross_target: float = 100e6\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enhanced Value/Growth with ML‐derived weights, rank features, borrow‐cost penalty,\n",
    "    sector‐ and market‐neutrality, and $100MM gross exposure target,\n",
    "    \"\"\"\n",
    "\n",
    "    # Invert ratios where higher = more value\n",
    "    enhanced_metrics['pe_ratio_inv'] = 1 / enhanced_metrics['pe1']\n",
    "    enhanced_metrics['pb_ratio_inv'] = 1 / enhanced_metrics['pb']\n",
    "    enhanced_metrics['evebitda_inv'] = 1 / enhanced_metrics['evebitda']\n",
    "    enhanced_metrics['de_inv'] = 1 / (enhanced_metrics['de'] + 1) \n",
    "    current_data = enhanced_metrics[enhanced_metrics['date'] == date].copy()\n",
    "\n",
    "    feat_cols = ['pe_ratio_inv', 'pb_ratio_inv', 'evebitda_inv', 'de_inv', \"divyield\",\"fcfps\", 'roic', 'roa', 'roe', 'currentratio', 'grossmargin'\n",
    "    ]\n",
    "\n",
    "     # Winsorize and fill missing\n",
    "    for col in feat_cols:\n",
    "        lo, hi = current_data[col].quantile([0.05,0.95])\n",
    "        current_data[col] = current_data[col].clip(lo, hi)\n",
    "        med = current_data[col].median()\n",
    "        current_data[col] = current_data[col].fillna(med)\n",
    "    \n",
    "    # Drop any rows missing the features\n",
    "    current_data.dropna(subset=feat_cols, inplace=True)\n",
    "\n",
    "    # Merge borrow costs\n",
    "    bc = borrow_costs_df[borrow_costs_df['date'] == date][['#SYM', 'borrow_cost']]\n",
    "    current_data = pd.merge(\n",
    "    current_data,\n",
    "    bc,\n",
    "    left_on='ticker',\n",
    "    right_on='#SYM',\n",
    "    how='left'\n",
    "    ).fillna({'borrow_cost': 0.01})\n",
    "\n",
    "    # Build historical training set\n",
    "    start_date = date - pd.DateOffset(months=lookback_months)\n",
    "    hist = enhanced_metrics[\n",
    "        (enhanced_metrics['date'] >= start_date) &\n",
    "        (enhanced_metrics['date'] <  date)\n",
    "    ].dropna(subset=feat_cols + ['return_1m_forward'])\n",
    "    if hist['date'].nunique() < 6:\n",
    "        raise ValueError(\"Not enough history to train ML model\")\n",
    "\n",
    "    # Cross‐sectional ranks in history\n",
    "    rank_cols = []\n",
    "    for col in feat_cols:\n",
    "        rcol = f\"{col}_rank\"\n",
    "        hist[rcol] = hist.groupby('date')[col].rank(pct=True)\n",
    "        rank_cols.append(rcol)\n",
    "\n",
    "    X = hist[rank_cols]\n",
    "    y = hist['return_1m_forward']\n",
    "\n",
    "    # Train with time‐series CV\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    model = RidgeCV(alphas=np.logspace(-3, 2, 10), cv=tscv)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Compute ranks on current date and predict\n",
    "    for col in feat_cols:\n",
    "        current_data[f\"{col}_rank\"] = current_data[col].rank(pct=True)\n",
    "    X_curr = current_data[[f\"{col}_rank\" for col in feat_cols]]\n",
    "\n",
    "    current_data['ml_score']  = model.predict(X_curr)\n",
    "    current_data['adj_score'] = current_data['ml_score'] - current_data['borrow_cost'] * 20\n",
    "\n",
    "    # Generate long/short signals via quintile bins\n",
    "    if sector_neutrality and 'sector' in current_data.columns:\n",
    "        pieces = []\n",
    "        for sector, grp in current_data.groupby('sector'):\n",
    "            grp = grp.copy()\n",
    "            grp['bin'] = pd.qcut(grp['adj_score'], 5, labels=False, duplicates='drop')\n",
    "            grp['pos'] = 0\n",
    "            grp.loc[grp['bin'] == 4, 'pos'] =  1\n",
    "            grp.loc[grp['bin'] == 0, 'pos'] = -1\n",
    "            pieces.append(grp)\n",
    "        pos_df = pd.concat(pieces)\n",
    "    else:\n",
    "        current_data =current_data.copy()\n",
    "        current_data['bin'] = pd.qcut(current_data['adj_score'], 5, labels=False, duplicates='drop')\n",
    "        current_data['pos'] = 0\n",
    "        current_data.loc[current_data['bin'] == 4, 'pos'] =  1\n",
    "        current_data.loc[current_data['bin'] == 0, 'pos'] = -1\n",
    "        pos_df = current_data\n",
    "\n",
    "    # Enforce max_positions per side\n",
    "    longs  = pos_df[pos_df.pos ==  1].nlargest(max_positions//2, 'adj_score')\n",
    "    shorts = pos_df[pos_df.pos == -1].nsmallest(max_positions//2, 'adj_score')\n",
    "    pos_df['pos'] = 0\n",
    "    pos_df.loc[longs.index,  'pos'] =  1\n",
    "    pos_df.loc[shorts.index, 'pos'] = -1\n",
    "\n",
    "    # Equal‐weight each side so sum(weights[pos==1])=+1, sum(weights[pos==-1])=-1\n",
    "    n_long  = (pos_df.pos ==  1).sum()\n",
    "    n_short = (pos_df.pos == -1).sum()\n",
    "    pos_df['weight'] = 0.0\n",
    "    if n_long:\n",
    "        pos_df.loc[pos_df.pos ==  1, 'weight'] =  1.0 / n_long\n",
    "    if n_short:\n",
    "        pos_df.loc[pos_df.pos == -1, 'weight'] = -1.0 / n_short\n",
    "\n",
    "    # Scale to $100MM gross exposure\n",
    "    current_gross = pos_df['weight'].abs().sum()\n",
    "    scale = gross_target / current_gross\n",
    "    pos_df['dollar_position'] = pos_df['weight'] * scale\n",
    "    pos_df['weight'] = pos_df['dollar_position'] / gross_target\n",
    "    pos_df.rename(columns={'pos': 'position'}, inplace=True)\n",
    "    pos_df = beta_neutralize(\n",
    "    pos_df=pos_df,\n",
    "    beta_df=beta_df,   # must have columns ['date','ticker','beta']\n",
    "    date=date,\n",
    "    gross_target=gross_target\n",
    "    )\n",
    "\n",
    "    return pos_df[['ticker','adj_score','bin','position','weight','dollar_position']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9e3e7",
   "metadata": {},
   "source": [
    "### Step 4: Run Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8805eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_trading_signals(\n",
    "    current_positions: pd.DataFrame, \n",
    "    previous_positions: Dict[str, Tuple[datetime, int]], \n",
    "    min_holding_days: int = 5\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Tuple[datetime, int]]]:\n",
    "    \"\"\"\n",
    "    Generate trading signals based on current and previous positions,\n",
    "    enforcing minimum holding period constraint.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    current_positions : pd.DataFrame\n",
    "    previous_positions : Dict[str, Tuple[datetime, int]]\n",
    "    min_holding_days : int, default=5\n",
    "        Minimum holding period in days (constraint from case study)\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    Tuple containing:\n",
    "      - trades: pd.DataFrame\n",
    "      - updated_positions: Dict[str, Tuple[datetime, int]]\n",
    "    \"\"\"\n",
    "    today = current_positions['date'].iloc[0]\n",
    "    trades = current_positions.copy()\n",
    "    trades['trade'] = 'HOLD'  # Default to HOLD\n",
    "    \n",
    "    # Initialize updated positions with previous positions\n",
    "    updated_positions = previous_positions.copy()\n",
    "    \n",
    "    for _, row in trades.iterrows():\n",
    "        ticker = row['ticker']\n",
    "        new_position = row['position']\n",
    "        \n",
    "        if ticker in previous_positions:\n",
    "            entry_date, old_position = previous_positions[ticker]\n",
    "            holding_period = (today - entry_date).days\n",
    "            \n",
    "            # Check if minimum holding period satisfied\n",
    "            if holding_period < min_holding_days:\n",
    "                # Can't exit position yet, override with previous position\n",
    "                trades.loc[trades['ticker'] == ticker, 'position'] = old_position\n",
    "                trades.loc[trades['ticker'] == ticker, 'trade'] = 'HOLD'\n",
    "                updated_positions[ticker] = (entry_date, old_position)\n",
    "            else:\n",
    "                # Can trade\n",
    "                if new_position != old_position:\n",
    "                    if new_position == 0:\n",
    "                        trades.loc[trades['ticker'] == ticker, 'trade'] = 'SELL'\n",
    "                        # Remove from positions if exiting\n",
    "                        if ticker in updated_positions:\n",
    "                            del updated_positions[ticker]\n",
    "                    else:\n",
    "                        trades.loc[trades['ticker'] == ticker, 'trade'] = 'MODIFY'\n",
    "                        # Update position\n",
    "                        updated_positions[ticker] = (today, new_position)\n",
    "                else:\n",
    "                    # Same position, just hold\n",
    "                    updated_positions[ticker] = (entry_date, old_position)\n",
    "        else:\n",
    "            # New position\n",
    "            if new_position != 0:\n",
    "                trades.loc[trades['ticker'] == ticker, 'trade'] = 'BUY'\n",
    "                updated_positions[ticker] = (today, new_position)\n",
    "    \n",
    "    # Recalculate weights after enforcing minimum holding period\n",
    "    long_count = (trades['position'] == 1).sum()\n",
    "    short_count = (trades['position'] == -1).sum()\n",
    "    \n",
    "    if long_count > 0:\n",
    "        trades.loc[trades['position'] == 1, 'weight'] = 1.0 / long_count\n",
    "    \n",
    "    if short_count > 0:\n",
    "        trades.loc[trades['position'] == -1, 'weight'] = -1.0 / short_count\n",
    "    \n",
    "    trades.loc[trades['position'] == 0, 'weight'] = 0\n",
    "    trades['dollar_position'] = trades['weight'] * 100000000\n",
    "    \n",
    "    return trades, updated_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47251761",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backtest_strategy(\n",
    "    value_metrics: pd.DataFrame,\n",
    "    borrow_costs_df: pd.DataFrame,\n",
    "    prices_df: pd.DataFrame,\n",
    "    strategy: Literal['traditional', 'enhanced'] = 'enhanced',\n",
    "    min_tickers_per_day: int = 100,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Backtest the value/growth strategy through time.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    value_metrics : pd.DataFrame\n",
    "    borrow_costs_df : pd.DataFrame\n",
    "    prices_df : pd.DataFrame\n",
    "    strategy : Literal['traditional', 'enhanced'], default='enhanced'\n",
    "        Strategy to use:\n",
    "        - 'traditional': Simple value factor approach\n",
    "        - 'enhanced': ML-enhanced approach with more features\n",
    "    min_tickers_per_day : int, default=100\n",
    "        Minimum number of tickers required for a trading day to be valid\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    Tuple containing three DataFrames:\n",
    "      - returns_df : pd.DataFrame      \n",
    "      - trades_df : pd.DataFrame\n",
    "      - positions_df : pd.DataFrame\n",
    "    \"\"\"\n",
    "    print(f'Running {strategy} strategy backtest')\n",
    "    trading_dates = sorted(value_metrics.groupby('date')['ticker'].nunique().reset_index().query(\"ticker>=100\")['date'].unique())\n",
    "\n",
    "    # Initialize results containers\n",
    "    daily_returns = []\n",
    "    positions_history = []\n",
    "    trade_history = []\n",
    "    \n",
    "    # Initialize position tracking\n",
    "    previous_positions = {}  # {ticker: (entry_date, position)}\n",
    "    \n",
    "    for i, current_date in enumerate(trading_dates):\n",
    "        print(f\"Processing {current_date.strftime('%Y-%m-%d')} ({i+1}/{len(trading_dates)})\")\n",
    "        \n",
    "        # Get positions for current date based on strategy\n",
    "        if strategy == 'traditional':\n",
    "            current_positions = traditional_value_growth_factor(value_metrics, current_date)\n",
    "        else:\n",
    "            current_positions = enhanced_value_growth_factor(value_metrics, borrow_costs_df, prices_df[['date','ticker','beta']], current_date)\n",
    "        \n",
    "        if len(current_positions) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Add date column\n",
    "        current_positions['date'] = current_date\n",
    "        \n",
    "        # Generate trading signals\n",
    "        if i > 0:\n",
    "            trades, previous_positions = generate_trading_signals(current_positions, previous_positions)\n",
    "        else:\n",
    "            # First day, all positions are new\n",
    "            trades = current_positions.copy()\n",
    "            trades['trade'] = 'BUY'\n",
    "            for _, row in trades.iterrows():\n",
    "                if row['position'] != 0:\n",
    "                    previous_positions[row['ticker']] = (current_date, row['position'])\n",
    "        \n",
    "        # Store positions and trades\n",
    "        positions_history.append(current_positions)\n",
    "        trade_history.append(trades)\n",
    "        \n",
    "        # Calculate 1-day forward returns if not the last day\n",
    "        if i < len(trading_dates) - 1:\n",
    "            next_date = trading_dates[i+1]\n",
    "            \n",
    "            # Get prices for current and next date\n",
    "            current_prices = prices[prices['date'] == current_date.strftime('%Y-%m-%d')][['ticker', 'close']].set_index('ticker')\n",
    "            next_prices = prices[prices['date'] == next_date.strftime('%Y-%m-%d')][['ticker', 'close']].set_index('ticker')\n",
    "            \n",
    "            # Calculate returns for positions\n",
    "            portfolio_return = 0\n",
    "            for ticker, weight in zip(trades['ticker'], trades['weight']):\n",
    "                if ticker in current_prices.index and ticker in next_prices.index:\n",
    "                    price_return = (next_prices.loc[ticker, 'close'] / current_prices.loc[ticker, 'close']) - 1\n",
    "                    portfolio_return += price_return * weight\n",
    "            \n",
    "            # Apply borrow costs for short positions (assuming annual cost, need to convert to daily)\n",
    "            if 'borrow_cost' in trades.columns:\n",
    "                for _, row in trades[trades['position'] == -1].iterrows():\n",
    "                    ticker = row['ticker']\n",
    "                    weight = row['weight']\n",
    "                    borrow_cost = row.get('borrow_cost', 0.01)  # Default to 1% if missing\n",
    "                    daily_borrow_cost = borrow_cost / 252  # Convert annual to daily\n",
    "                    portfolio_return -= daily_borrow_cost * abs(weight)\n",
    "            \n",
    "            daily_returns.append({\n",
    "                'date': next_date,\n",
    "                'return': portfolio_return\n",
    "            })\n",
    "    \n",
    "    # Combine results\n",
    "    if len(daily_returns) > 0:\n",
    "        returns_df = pd.DataFrame(daily_returns)\n",
    "        \n",
    "        # Calculate cumulative return\n",
    "        returns_df['cumulative_return'] = (1 + returns_df['return']).cumprod() - 1\n",
    "        \n",
    "        # Calculate rolling measures\n",
    "        returns_df['rolling_vol_30d'] = returns_df['return'].rolling(30).std() * np.sqrt(252)\n",
    "        returns_df['rolling_sharpe_30d'] = (returns_df['return'].rolling(30).mean() * 252) / \\\n",
    "                                         (returns_df['return'].rolling(30).std() * np.sqrt(252))\n",
    "        \n",
    "        # Calculate drawdowns\n",
    "        returns_df['peak'] = returns_df['cumulative_return'].cummax()\n",
    "        returns_df['drawdown'] = (returns_df['cumulative_return'] - returns_df['peak']) / (1 + returns_df['peak'])\n",
    "    else:\n",
    "        returns_df = pd.DataFrame(columns=['date', 'return', 'cumulative_return', 'drawdown'])\n",
    "    \n",
    "    # Combine positions and trades\n",
    "    if positions_history:\n",
    "        positions_df = pd.concat(positions_history, ignore_index=True)\n",
    "    else:\n",
    "        positions_df = pd.DataFrame()\n",
    "    \n",
    "    if trade_history:\n",
    "        trades_df = pd.concat(trade_history, ignore_index=True)\n",
    "    else:\n",
    "        trades_df = pd.DataFrame()\n",
    "    \n",
    "    return returns_df, trades_df, positions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1051d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_sector_information(\n",
    "    trades_df: pd.DataFrame,\n",
    "    sectors_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add sector information to trade data using merge_asof.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    trades_df : pd.DataFrame\n",
    "    sectors_df : pd.DataFrame\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Trade data with sector information added\n",
    "    \"\"\"\n",
    "    # Prepare sector data for merging\n",
    "    sectors_with_dates = sectors_df.copy()\n",
    "    sectors_with_dates['effective_date'] = pd.to_datetime(sectors_with_dates['lastupdated'])\n",
    "    sectors_with_dates = sectors_with_dates.sort_values(['effective_date'])\n",
    "    \n",
    "    # Sort trades for merge_asof\n",
    "    trades_sorted = trades_df.sort_values(['date'])\n",
    "    \n",
    "    # Merge sectors into trades data\n",
    "    trades_with_sectors = pd.merge_asof(\n",
    "        trades_sorted,\n",
    "        sectors_with_dates[['ticker', 'sector', 'effective_date']],\n",
    "        left_on='date',\n",
    "        right_on='effective_date',\n",
    "        by='ticker',\n",
    "        direction='nearest'\n",
    "    )\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    trades_with_sectors.drop(columns=['effective_date'], inplace=True)\n",
    "    \n",
    "    return trades_with_sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036212ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_backtest_results(\n",
    "    returns_df: pd.DataFrame,\n",
    "    trades_df: pd.DataFrame,\n",
    "    positions_df: pd.DataFrame,\n",
    "    strategy_name: str,\n",
    "    output_dir: Optional[str] = None\n",
    ") -> Dict[str, Path]:\n",
    "    \"\"\"\n",
    "    Save backtest results to CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    returns_df : pd.DataFrame\n",
    "        DataFrame containing strategy returns\n",
    "    trades_df : pd.DataFrame\n",
    "        DataFrame containing trade information\n",
    "    positions_df : pd.DataFrame\n",
    "        DataFrame containing position information\n",
    "    strategy_name : str\n",
    "        Name of the strategy (used in filenames)\n",
    "    output_dir : Optional[str], default=None\n",
    "        Directory to save files (defaults to current directory)\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    Dict[str, Path]\n",
    "        Dictionary of saved file paths\n",
    "    \"\"\"\n",
    "    # Set up output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = '.'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create safe strategy name for filenames\n",
    "    safe_name = strategy_name.lower().replace(' ', '_').replace('/', '_')\n",
    "    \n",
    "    # Define file paths\n",
    "    returns_path = os.path.join(output_dir, f'returns_{safe_name}.csv')\n",
    "    trades_path = os.path.join(output_dir, f'trades_{safe_name}.csv')\n",
    "    positions_path = os.path.join(output_dir, f'positions_{safe_name}.csv')\n",
    "    \n",
    "    # Save files\n",
    "    returns_df.to_csv(returns_path, index=None)\n",
    "    trades_df.to_csv(trades_path, index=None)\n",
    "    positions_df.to_csv(positions_path, index=None)\n",
    "    \n",
    "    # Return paths for reference\n",
    "    return {\n",
    "        'returns': Path(returns_path),\n",
    "        'trades': Path(trades_path),\n",
    "        'positions': Path(positions_path)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce89e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_strategy_backtest(\n",
    "    metrics_data: pd.DataFrame,\n",
    "    borrow_fees_data: pd.DataFrame,\n",
    "    prices_df: pd.DataFrame,\n",
    "    sectors_data: pd.DataFrame,\n",
    "    strategy_type: str,\n",
    "    strategy_name: Optional[str] = None,\n",
    "    output_dir: Optional[str] = None\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Run a complete backtest for a strategy, including adding sector data and saving results.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    metrics_data : pd.DataFrame\n",
    "        Value metrics data for the backtest\n",
    "    borrow_fees_data : pd.DataFrame\n",
    "        Borrow fees data\n",
    "    sectors_data : pd.DataFrame\n",
    "        Sector classification data\n",
    "    strategy_type : str\n",
    "        Type of strategy ('traditional' or 'enhanced')\n",
    "    strategy_name : Optional[str], default=None\n",
    "        Name for the strategy (defaults to the strategy_type)\n",
    "    output_dir : Optional[str], default=None\n",
    "        Directory to save results (defaults to current directory)\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n",
    "        Tuple containing (returns_df, trades_df, positions_df)\n",
    "    \"\"\"\n",
    "    # Set default strategy name if not provided\n",
    "    if strategy_name is None:\n",
    "        strategy_name = strategy_type.capitalize()\n",
    "    \n",
    "    # Run backtest\n",
    "    print(f'Running {strategy_type} strategy backtest')\n",
    "    returns_df, trades_df, positions_df = backtest_strategy(\n",
    "        metrics_data, borrow_fees_data, prices_df, strategy_type\n",
    "    )\n",
    "    \n",
    "    # Add sector information to trades\n",
    "    trades_df_with_sectors = add_sector_information(trades_df, sectors_data)\n",
    "    \n",
    "    # Save results\n",
    "    save_backtest_results(\n",
    "        returns_df, \n",
    "        trades_df_with_sectors, \n",
    "        positions_df, \n",
    "        strategy_name,\n",
    "        output_dir\n",
    "    )\n",
    "    \n",
    "    return returns_df, trades_df_with_sectors, positions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f2acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Run Traditional Strategy Backtest')\n",
    "# Run traditional strategy backtest\n",
    "returns_trad_df, trades_trad_df, positions_trad_df = run_strategy_backtest(\n",
    "    value_metrics, \n",
    "    borrow_fees,\n",
    "    prices,\n",
    "    sectors_df, \n",
    "    'traditional',\n",
    "    'Traditional Value/Growth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ad03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_beta(prices: pd.DataFrame, \n",
    "                          spx_path: str = 'SP500.csv', \n",
    "                          window: int = 252, \n",
    "                          min_periods: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate rolling beta for each ticker using SPX as the market benchmark.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prices : DataFrame\n",
    "        DataFrame with columns ['ticker', 'date', 'close']\n",
    "    spx_path : str, default='SP500.csv'\n",
    "        Path to S&P 500 data CSV file\n",
    "    window : int, default=252\n",
    "        Rolling window for beta calculation (trading days)\n",
    "    min_periods : int, default=20\n",
    "        Minimum number of observations required for beta calculation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Original prices DataFrame with additional columns: 'spx_return', 'return', and 'beta'\n",
    "    \"\"\"\n",
    "    # Create a copy of the input dataframe to avoid modifying the original\n",
    "    prices_df = prices.copy()\n",
    "    \n",
    "    # Get date range\n",
    "    start_date = prices_df['date'].min()\n",
    "    end_date = prices_df['date'].max()\n",
    "    \n",
    "    # Load SPX data (alternatively could use yfinance if rate limits not an issue which is something I ran into and hd to use the csv file)\n",
    "    spx_df = pd.read_csv(spx_path).dropna()\n",
    "    spx_df.rename(columns={'SP500': 'spx_close', 'observation_date': 'date'}, inplace=True)\n",
    "    spx_df = spx_df[(spx_df['date'] >= start_date) & (spx_df['date'] <= end_date)].copy()\n",
    "    \n",
    "    # Calculate SPX returns\n",
    "    spx_df[\"spx_return\"] = spx_df[\"spx_close\"].pct_change()\n",
    "    spx_df = spx_df[[\"date\", \"spx_return\"]]\n",
    "    \n",
    "    # Merge SPX returns into prices and compute each ticker's return\n",
    "    prices_df = prices_df.merge(spx_df, on=\"date\", how=\"left\")\n",
    "    prices_df[\"return\"] = prices_df.groupby(\"ticker\")[\"close\"].pct_change()\n",
    "    \n",
    "    # Define a function that fits a rolling OLS and pulls out β\n",
    "    def add_rolling_beta(group):\n",
    "        # drop NAs and index by date\n",
    "        g = group.dropna(subset=[\"return\", \"spx_return\"]).set_index(\"date\")\n",
    "        if len(g) < min_periods:\n",
    "            group[\"beta\"] = np.nan\n",
    "            return group\n",
    "        Y = g[\"return\"]\n",
    "        X = sm.add_constant(g[\"spx_return\"])\n",
    "        window_tick = min(len(X), window)\n",
    "        rols = RollingOLS(Y, X, window=window_tick)\n",
    "        rres = rols.fit()\n",
    "        g[\"beta\"] = rres.params[\"spx_return\"]\n",
    "        return g.reset_index()\n",
    "    \n",
    "    # Apply per ticker\n",
    "    prices_with_beta = (\n",
    "        prices_df\n",
    "        .groupby(\"ticker\", group_keys=False)\n",
    "        .apply(add_rolling_beta)\n",
    "    )\n",
    "    \n",
    "    return prices_with_beta\n",
    "\n",
    "prices_with_beta = calculate_rolling_beta(prices)\n",
    "prices_with_beta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f97398",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Run Enhanced Strategy Backtest')\n",
    "# Run enhanced strategy backtest\n",
    "returns_enhanced_df, trades_enhanced_df, positions_enhanced_df = run_strategy_backtest(\n",
    "    enhanced_metrics, \n",
    "    borrow_fees,\n",
    "    prices_with_beta,\n",
    "    sectors_df, \n",
    "    'enhanced',\n",
    "    'Enhanced Value/Growth'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6cba4",
   "metadata": {},
   "source": [
    "### Step 5: Analyze and Visualize Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa165cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_performance(returns_df: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Analyze strategy performance and calculate key risk/return metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    returns_df : pd.DataFrame\n",
    "        Returns data from backtest\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        Dictionary of performance metrics\n",
    "    \"\"\"\n",
    "    if len(returns_df) == 0:\n",
    "        return {'error': 'No returns data available'}\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    total_days = len(returns_df)\n",
    "    trading_days_per_year = 252\n",
    "    years = total_days / trading_days_per_year\n",
    "    \n",
    "    # Return metrics\n",
    "    total_return = returns_df['cumulative_return'].iloc[-1]\n",
    "    annualized_return = (1 + total_return) ** (1 / years) - 1\n",
    "    \n",
    "    # Risk metrics\n",
    "    daily_vol = returns_df['return'].std()\n",
    "    annualized_vol = daily_vol * np.sqrt(trading_days_per_year)\n",
    "    sharpe_ratio = annualized_return / annualized_vol if annualized_vol > 0 else 0\n",
    "    \n",
    "    # Drawdown metrics\n",
    "    max_drawdown = returns_df['drawdown'].min()\n",
    "    \n",
    "    # Downside metrics\n",
    "    downside_returns = returns_df[returns_df['return'] < 0]['return']\n",
    "    downside_vol = downside_returns.std() * np.sqrt(trading_days_per_year) if len(downside_returns) > 0 else 0\n",
    "    sortino_ratio = annualized_return / downside_vol if downside_vol > 0 else 0\n",
    "    \n",
    "    # Win/loss metrics\n",
    "    win_days = (returns_df['return'] > 0).sum()\n",
    "    loss_days = (returns_df['return'] < 0).sum()\n",
    "    win_rate = win_days / total_days if total_days > 0 else 0\n",
    "    \n",
    "    \n",
    "    metrics = {\n",
    "        'total_return': total_return,\n",
    "        'annualized_return': annualized_return,\n",
    "        'annualized_volatility': annualized_vol,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'win_rate': win_rate,\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f15395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_performance(\n",
    "    returns_df: pd.DataFrame,\n",
    "    metrics: Dict[str, float],\n",
    "    trades_df: Optional[pd.DataFrame] = None,\n",
    "    strategy_name: str = 'Enhanced Value/Growth',\n",
    "    output_dir: Optional[str] = None,\n",
    "    show_plots: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualize strategy performance through multiple plots.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    returns_df : pd.DataFrame\n",
    "    metrics : Dict[str, float]\n",
    "    trades_df : Optional[pd.DataFrame], default=None\n",
    "    strategy_name : str, default='Enhanced Value/Growth'\n",
    "    output_dir : Optional[str], default=None    \n",
    "    show_plots : bool, default=True\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    None\n",
    "        This function generates and saves plots but doesn't return any values.\n",
    "    \"\"\"\n",
    "    if len(returns_df) == 0:\n",
    "        print(\"No data to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Set up figure\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # 1. Cumulative Returns\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(returns_df['date'], returns_df['cumulative_return'] * 100, linewidth=2)\n",
    "    plt.title(f'{strategy_name} Cumulative Returns (%)', fontsize=14)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Return (%)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add key metrics as text\n",
    "    metrics_text = (\n",
    "        f\"Annual Return: {metrics['annualized_return']:.2%}\\n\"\n",
    "        f\"Sharpe Ratio: {metrics['sharpe_ratio']:.2f}\\n\"\n",
    "        f\"Max Drawdown: {metrics['max_drawdown']:.2%}\"\n",
    "    )\n",
    "    plt.annotate(metrics_text, xy=(0.02, 0.85), xycoords='axes fraction', fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 2. Drawdowns\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.fill_between(returns_df['date'], returns_df['drawdown'] * 100, 0, color='red', alpha=0.3)\n",
    "    plt.title(f'{strategy_name} Drawdowns (%)', fontsize=14)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Drawdown (%)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Rolling Volatility\n",
    "    if 'rolling_vol_30d' in returns_df.columns:\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(returns_df['date'], returns_df['rolling_vol_30d'] * 100, color='purple', linewidth=2)\n",
    "        plt.title('30-Day Rolling Volatility (%)', fontsize=14)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Annualized Volatility (%)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Rolling Sharpe Ratio\n",
    "    if 'rolling_sharpe_30d' in returns_df.columns:\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(returns_df['date'], returns_df['rolling_sharpe_30d'], color='green', linewidth=2)\n",
    "        plt.title('30-Day Rolling Sharpe Ratio', fontsize=14)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Sharpe Ratio')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{strategy_name.replace(\"/\", \"_\")}_performance.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional plots for deeper analysis\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    # Return distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(returns_df['return'] * 100, kde=True, bins=50)\n",
    "    plt.title('Daily Return Distribution (%)', fontsize=14)\n",
    "    plt.xlabel('Daily Return (%)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Position analysis if trade data is available\n",
    "    if trades_df is not None and len(trades_df) > 0:\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        \n",
    "        # Position count over time\n",
    "        position_counts = (\n",
    "        trades_df\n",
    "        .groupby(['date', 'position'])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "        .rename(columns={1: 'long', -1: 'short'})\n",
    "        .reset_index()\n",
    "    )\n",
    "        \n",
    "        # Create 1×2 subplots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "        # Long positions chart\n",
    "        axes[0].bar(position_counts['date'], position_counts['long'])\n",
    "        axes[0].set_title('Long Positions Over Time', fontsize=14)\n",
    "        axes[0].set_xlabel('Date')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        axes[0].grid(True, alpha=0.7)\n",
    "\n",
    "        # Short positions chart\n",
    "        axes[1].bar(position_counts['date'], position_counts['short'], color='green')\n",
    "        axes[1].set_title('Short Positions Over Time', fontsize=14)\n",
    "        axes[1].set_xlabel('Date')\n",
    "        axes[1].grid(True, alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Sector distribution if sector data available\n",
    "        if 'sector' in trades_df.columns:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            latest_positions = trades_df[trades_df['date'] == trades_df['date'].max()]\n",
    "            \n",
    "            long_sectors = latest_positions[latest_positions['position'] == 1]['sector'].value_counts()\n",
    "            short_sectors = latest_positions[latest_positions['position'] == -1]['sector'].value_counts()\n",
    "            \n",
    "            sector_exposure = pd.DataFrame({\n",
    "                'long': long_sectors,\n",
    "                'short': short_sectors\n",
    "            }).fillna(0)\n",
    "            \n",
    "            sector_exposure.plot(kind='barh', color=['green', 'red'])\n",
    "            plt.title('Latest Sector Distribution', fontsize=14)\n",
    "            plt.xlabel('Number of Positions')\n",
    "            plt.ylabel('Sector')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{strategy_name.replace(\"/\", \"_\")}_position_analysis.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd0b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Analyzing performance...\")\n",
    "metrics = analyze_performance(returns_trad_df)\n",
    "strategy='traditional'\n",
    "\n",
    "print(f\"\\nPerformance Summary - {strategy.capitalize()} Strategy:\")\n",
    "print(f\"Total Return: {metrics['total_return']:.2%}\")\n",
    "print(f\"Annualized Return: {metrics['annualized_return']:.2%}\")\n",
    "print(f\"Annualized Volatility: {metrics['annualized_volatility']:.2%}\")\n",
    "print(f\"Sharpe Ratio: {metrics['sharpe_ratio']:.2f}\")\n",
    "print(f\"Maximum Drawdown: {metrics['max_drawdown']:.2%}\")\n",
    "print(f\"Win Rate: {metrics['win_rate']:.2%}\")\n",
    "\n",
    "\n",
    "print(f\"\\nVisualizing results...\")\n",
    "visualize_performance(returns_trad_df, metrics, trades_trad_df, f\"{strategy.capitalize()} Value/Growth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Analyzing performance...\")\n",
    "metrics_enhanced = analyze_performance(returns_enhanced_df)\n",
    "strategy='enhanced'\n",
    "\n",
    "print(f\"\\nPerformance Summary - {strategy.capitalize()} Strategy:\")\n",
    "print(f\"Total Return: {metrics_enhanced['total_return']:.2%}\")\n",
    "print(f\"Annualized Return: {metrics_enhanced['annualized_return']:.2%}\")\n",
    "print(f\"Annualized Volatility: {metrics_enhanced['annualized_volatility']:.2%}\")\n",
    "print(f\"Sharpe Ratio: {metrics_enhanced['sharpe_ratio']:.2f}\")\n",
    "print(f\"Maximum Drawdown: {metrics_enhanced['max_drawdown']:.2%}\")\n",
    "print(f\"Win Rate: {metrics_enhanced['win_rate']:.2%}\")\n",
    "\n",
    "\n",
    "print(f\"\\nVisualizing results...\")\n",
    "visualize_performance(returns_enhanced_df, metrics_enhanced, trades_enhanced_df, f\"{strategy.capitalize()} Value/Growth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8bcd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mongrove",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
